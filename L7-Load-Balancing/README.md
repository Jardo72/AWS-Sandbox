# L7 Load-Balancing Demo

## Introduction
L7 Load-Balancing Demo is an experimantal/educational application meant as illustration of L7 (aka application) load balancing in combination with AWS, Docker, Kubernetes etc. It is a Java application based on the SpringBoot framework which exposes 3 groups of REST API endpoints:
- Load-Balancing Demonstration API
- Health Check API
- CPU Consumption API

Details for each of the three API endpoint groups are described below. In order to submit REST requests to the application, use a tool like [Postman](https://www.postman.com) or [curl](https://curl.se). Unless you override the default configuration, the application is listening on port 80, so you do not need to specify the port in the URLs. As the application does not deal with any sensitive data, there is no need for security. In addition, I wanted to make the application as simple and cheap as possible. Therefore, the REST API exposed by the application is accessible via plain HTTP (i.e. not via HTTPS).


#### Load-Balancing Demonstration API
Load-balancing demonstration API involves single API endpoint (`GET /api/system-info`) that can be used to illustrate various aspects of load balancing. The response returned by this API endpoint involves the following information:
- The number of REST requests handled so far by the application instance. There is a counter which is incremented whenever this API endpoint is involved, and there are similar counters for the health check API endpoint and the CPU consumption API endpoint.
- Information about the server process and the host where it is running (hostname, username, number of processors available).
- Information about the connection used to deliver the current request (e.g. IP address/TCP port for both client and server). In case of L7 load balancing, the client IP address is in fact the IP address of the load balancer (not the client the request originates from).
- All HTTP headers present in the request. The purpose is to illustrate the `X-Forwarded-*` headers which are added by some load balancers like AWS ALB.

The following JSON snippet illustrates the response generated by this endpoint (the application was running on an AWS EC2 instance in this particular case).

```json
{
    "serverInformation": {
        "username": "root",
        "hostname": "ip-10-0-0-91.eu-central-1.compute.internal",
        "availableProcessors": 1
    },
    "requestInformation": {
        "scheme": "http",
        "clientEndpoint": {
            "address": "10.0.2.174",
            "port": 22074
        },
        "serverEndpoint": {
            "address": "10.0.0.91",
            "port": 80
        },
        "httpHeaders": [
            {
                "name": "x-forwarded-for",
                "values": [
                    "178.41.31.158"
                ]
            },
            {
                "name": "x-forwarded-proto",
                "values": [
                    "http"
                ]
            },
            {
                "name": "x-forwarded-port",
                "values": [
                    "80"
                ]
            },
            {
                "name": "host",
                "values": [
                    "l7-lb-demo-loadbalancer-1472867745.eu-central-1.elb.amazonaws.com"
                ]
            },
            {
                "name": "x-amzn-trace-id",
                "values": [
                    "Root=1-61181337-12338f1d4020035d2964143a"
                ]
            },
            {
                "name": "user-agent",
                "values": [
                    "PostmanRuntime/7.28.3"
                ]
            },
            {
                "name": "accept",
                "values": [
                    "*/*"
                ]
            },
            {
                "name": "cache-control",
                "values": [
                    "no-cache"
                ]
            },
            {
                "name": "postman-token",
                "values": [
                    "503b5766-a2ad-453d-8cdf-c45f5b56d1cf"
                ]
            },
            {
                "name": "accept-encoding",
                "values": [
                    "gzip, deflate, br"
                ]
            }
        ],
        "secure": false
    },
    "statisticInformation": {
        "systemInfoRequestCount": 1,
        "healthCheckCount": 264,
        "cpuConsumptionRequestCount": 614
    }
}
```

The project also involves a simple client application written in Python that can be used to generate many `GET /api/system-info` requests. The client application consists of a single Python file ([system-info-client.py](./system-info-client.py)), and it expects 3 command line arguments:
- The hostname or IP address of the backend. If a load balancer is in front of the backend, the hostname or the IP address of the load balancer is to be used.
- The TCP port of the backend. If load balancer is used, the TCP port of the load balancer is to be used.
- The number of requests to be sent.

The application is single-threaded, so it sends the requests sequentially. After sending the specified number of requests, the client application prints the number of requests handled by particular backend instances. The following snippet illustrates the output generated by the client application.

```
--------------------------------------------------------------------------------
- Test Parameters
--------------------------------------------------------------------------------
Host:          L7-Load-Balancing-Demo-ALB-1572882261.eu-central-1.elb.amazonaws.com
Port:          80
Request count: 1000


--------------------------------------------------------------------------------
- Requests
--------------------------------------------------------------------------------
250 of 1000 requests completed...
500 of 1000 requests completed...
750 of 1000 requests completed...
1000 of 1000 requests completed...


--------------------------------------------------------------------------------
- Summary (statistics)
--------------------------------------------------------------------------------

Requests per server:
   172.31.17.149:     334
   172.31.35.165:     333
     172.31.9.20:     333

Requests per HTTP status code:
200:    1000
```

#### Health Check API
Health check API involves an API endpoint (`GET /api/health-check`) that is supposed to be used as load balancer health check. In addition, there are two additional endpoints that can be used to control the behavior of the above mentioned health check endpoint:
- `PUT /api/health-status?status=<STATUS>` allows to specify the outcome of subsequent invocations of the health check endpoint. The query string parameter `status` can have one of the following three values: the value `OK` (default) specifies that subsequent health checks will succeed with HTTP status 200; the value `ERROR` means that health checks will fail with HTTP status 500; and the value `HANG` will cause that subsequent health checks will block the thread handling the health check request forever (i.e. the load balancer health check will most likely fail with timeout).
- `GET /api/health-status` returns the current status (i.e. the value specified by the last PUT request).

#### CPU Consumption API
CPU consumption API involves just a single endpoint (`GET /api/consume-cpu`) that can be used to trigger a calculation leading to high CPU load. If this application is deployed to an AWS auto scaling group, this API endpoint can be used to demonstrate the auto scaling. The request has no parameters. Besides auto scaling, this API endpoint can also be used to demonstrate AWS CloudWatch metrics and alarms.

The project also involves a simple client application written in Python that can be used to generate many `GET /api/consume-cpu` requests. The client application consists of a single Python file ([cpu-consumption-client.py](./cpu-consumption-client.py)), and it expects 4 command line arguments:
- The hostname or IP address of the backend. If a load balancer is in front of the backend, the hostname or the IP address of the load balancer is to be used.
- The TCP port of the backend. If load balancer is used, the TCP port of the load balancer is to be used.
- Duration of the request generation in minutes.
- Number of threads to be used to generate requests.

## Source Code Organization and Building
The application is organized as a simple Maven project. In order to build the applications, just navigate to the root directory of the project and execute the following command (assumed Maven is installed and properly configured):

```
mvn clean package
```

The command above builds a fat runnable JAR file with all dependencies (including Spring and embedded HTTP server) which can be immediately used to start the application. The name of the JAR file is `aws-sandbox-application-load-balancing-server-1.0.jar`, and it resides in the `target` directory.

## How to Start the Service
The fat runnable JAR file mentioned in the previous section also involves `application.properties` file allowing to configure the application. The default configuration binds the embedded HTTP server to all available network interfaces (i.e. 0.0.0.0), so in the vast majority of cases, there is no need to change this. The default configuration binds the embedded HTTP server to the TCP port 80, which should also be OK for most use cases. The following command illustrates how to start the application with the default settings.

```
java -jar ./target/aws-sandbox-application-load-balancing-server-1.0.jar
```

The above mentioned default settings can be overwritten by Java system properties when starting the application. The following command illustrates how to bind the HTTP server to the network interface with the IP address 192.168.0.10 and to the TCP port 8080.

```
java -Dserver.address=192.168.0.10 -Dserver.port=8080 -jar ./target/aws-sandbox-application-load-balancing-server-1.0.jar
```

## How to Deploy the Application to AWS

### Application Load Balancer + EC2 Auto Scaling Group
The project involves parametrized CloudFormation template ([cloud-formation-template.yml](./cloud-formation-template.yml)) that will automatically create a setup with an application load balancer and an EC2 auto scaling group running several instances of the application. The CloudFormation template creates a complete stack with the following resources:
* Custom VPC with three public subnets (each in separate AZ), Internet Gateway and custom route table with a route to the Internet Gateway.
* Internet-facing application load balancer.
* Two security groups - one protecting the load balancer, the other protecting the EC2 instances.
* Launch template for the EC2 instances, with user data involving download of the application JAR from an S3 bucket. IAM instance profile allowing access to the S3 bucket is also created as part of the stack. The S3 bucket is not part of the stack - it must exist when the creation of the CloudFormation stack is started, and the application JAR file must available in the S3 bucket.
* EC2 auto scaling group with ELB health checks and target tracking scaling policy that dynamically adjusts the number of EC2 instances based on the CPU utilization.
* Optional S3 bucket serving as storage for ALB access log. This is created only if the access log is requested, which is driven by one of the template parameters.
* Optional Route 53 alias for the DNS name of the ALB. This is created only if explicitly requested by one of the template parameters.

The template defines mapping for AMI IDs, so the template can be used in various AWS regions. However, the mapping only contains AMI IDs for three regions: eu-central-1, eu-west-1 and eu-west-2. The following AWS CLI command illustrates how to use the CloudFormation template to create the stack.
```
aws cloudformation create-stack --stack-name L7-LB-Demo --template-body file://cloud-formation-template.yml --parameters file://stack-params.json --capabilities CAPABILITY_NAMED_IAM --on-failure ROLLBACK
```

The template involves several parameters. For some of them, default values are defined, so the values of these parameters can be omitted when creating the CloudFormation stack. However, some of the parameters require explicit values when creating the stack as there are no default values. The [stack-params.json](./stack-params.json) file contains parameter values used during my experiments. The template expects the application JAR file to be available on an S3 bucket. The name of the S3 bucket must be specified as one of the template parameters.

If the auto scaling group involves three instances of instance type t2.micro, using the [cpu-consumption-client.py](./cpu-consumption-client.py) script with three threads for 15 minutes is sufficient to achieve aggregate CPU utilization over 50%, which is the default threshold for scaling out. In other words, such a load is enough to force the auto scaling group to launch new EC2 instance(s) and thus new application instance(s).

When deleting the stack, the S3 bucket serving as storage for ALB access log will not be deleted if there are some log files. In such case, the deletion of the entire stack will fail. The most simple way to resolve the issue is to retry the removal of the stack and allow CloudFormation to retain the S3 bucket. You can remove the log files and the bucket manually afterwards. 
